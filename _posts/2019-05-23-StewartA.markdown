---
title: "Stewart, A. | Machine learning, and Artificial Intelligence"
layout: post
date: 2019-05-23 07:56
image:
headerImage: false
tag:
- keynote
- machine learning
- AI
category: abstracts
author:
#description: Markdown summary with different options
---

_**Andrew Stewart**_<br/>
Department of Physics & Bernal Institute, University of Limerick, Ireland.<br/>

## Abstract

Machine Learning (ML) and Artifical Intelligence (AI) appear to be combining with every aspect of science and technology over the past few years.  The promise of automated experiments and data analysis is a strong incentive and motivation to implement these technologies into a subject.
With self-driving cars, automatic detection of people and objects within images, it would appear to be a natural fit for transmission electron microscopy (TEM).  Being a predominately imaged based research field, we can benefit from the advances that ML and AI have already achieved in other research areas, and incorporate them into our field. The ability to automatically search data for features of interest has particular significance as we move to ever larger and faster detectors, at the high end we can already collecting up to 1 Terabyte of data in 20 minutes. These large data volumes are becoming more commonplace while capturing the dynamic nature of materials studied with in situ electron microscopy,  a rapidly developing area within the TEM community and the data volumes and more sophisticated analysis is only going to increase. Such extensive data are no longer feasible to analyse manually frame by frame in great detail the way we would traditionally assess individual micrographs.
There are now many freely available application programming interfaces (APIs) such as Tensorflow, Keras, Caffe, which make it relatively easy to implement neural networks (NN) or convolutional neural networks (CNN) the critical technology to ML and AI.  These APIs streamline the process of setting up models and metrics to automate a process. Further considerations include the computing hardware necessary to handle the data on reasonable time scales, how large does a training set need to be? Which statistical tests and metrics to use, and what is overfitting? All these aspects need to be understood before ML, and AI can be implemented effectively for TEM.
How can our field benefit and thrive from the integration of ML and AI, what are the limitations and drawbacks and how can we learn from the early adopters in other fields and our own? How to proceed with these exciting and powerful possibilities that ML and AI will bring to the TEM field will be discussed.<br/>
